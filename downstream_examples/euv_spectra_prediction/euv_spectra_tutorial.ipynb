{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# EUV Spectra Prediction Tutorial for Beginners 🌞\n",
        "\n",
        "This notebook will guide you through running EUV (Extreme Ultraviolet) spectra prediction using the Surya model. \n",
        "\n",
        "## What you'll learn:\n",
        "- How to load a pre-trained model for EUV spectra prediction\n",
        "- How to run inference on solar data to predict 1343-dimensional spectra\n",
        "- How to interpret spectral forecasting results\n",
        "- Understanding vector predictions vs scalar outputs\n",
        "- How to visualize and analyze EUV spectral data\n",
        "\n",
        "## Prerequisites:\n",
        "- Make sure you're in the correct directory: `downstream_examples/euv_spectra_prediction/`\n",
        "- Ensure all required packages are installed (torch, yaml, matplotlib, numpy, etc.)\n",
        "\n",
        "Let's get started! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/rhome/rlal/Surya/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All imports successful!\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Import functions from our inference script\n",
        "from infer import (\n",
        "    run_inference,\n",
        "    create_spectrum_plots\n",
        ")\n",
        "\n",
        "# Import from surya\n",
        "from surya.utils.data import build_scalers\n",
        "from surya.utils.distributed import set_global_seed\n",
        "\n",
        "print(\"✅ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Download Pre-trained Model Weights\n",
        "\n",
        "The model weights will be automatically downloaded from Hugging Face. This might take a few minutes on first run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Downloading model weights...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 1 files: 100%|██████████| 1/1 [00:05<00:00,  5.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model weights downloaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download model weights from Hugging Face\n",
        "print(\"📥 Downloading model weights...\")\n",
        "snapshot_download(\n",
        "    repo_id=\"nasa-ibm-ai4science/euv_spectra_surya\",\n",
        "    local_dir=\"./assets\",\n",
        "    allow_patterns='*.pth',\n",
        "    token=None,\n",
        ")\n",
        "print(\"✅ Model weights downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Up Configuration\n",
        "\n",
        "We need to load the configuration file that contains all the model and data parameters. Make sure you have a `config.yaml` file in your current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Loading configuration...\n",
            "✅ Configuration loaded successfully!\n",
            "Model type: spectformer\n",
            "Data precision: torch.float32\n",
            "Expected spectrum dimension: 1343\n"
          ]
        }
      ],
      "source": [
        "# Configuration paths - modify these if your files are in different locations\n",
        "config_path = \"./config.yaml\"\n",
        "checkpoint_path = \"./assets/euv_spectra_weights.pth\"\n",
        "output_dir = \"./inference_results\"\n",
        "\n",
        "# Set global seed for reproducibility\n",
        "set_global_seed(42)\n",
        "\n",
        "# Load configuration\n",
        "print(\"📋 Loading configuration...\")\n",
        "try:\n",
        "    config = yaml.safe_load(open(config_path, \"r\"))\n",
        "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
        "    print(\"✅ Configuration loaded successfully!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error: {e}\")\n",
        "    print(\"Make sure config.yaml exists in your current directory\")\n",
        "    raise\n",
        "\n",
        "# Set data type (float precision)\n",
        "if config[\"dtype\"] == \"float16\":\n",
        "    config[\"dtype\"] = torch.float16\n",
        "elif config[\"dtype\"] == \"bfloat16\":\n",
        "    config[\"dtype\"] = torch.bfloat16\n",
        "elif config[\"dtype\"] == \"float32\":\n",
        "    config[\"dtype\"] = torch.float32\n",
        "else:\n",
        "    raise NotImplementedError(\"Please choose from [float16,bfloat16,float32]\")\n",
        "\n",
        "print(f\"Model type: {config['model']['model_type']}\")\n",
        "print(f\"Data precision: {config['dtype']}\")\n",
        "print(f\"Expected spectrum dimension: 1343\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Set Up Device (GPU/CPU)\n",
        "\n",
        "Let's determine whether to use GPU or CPU for inference. GPU is much faster if available!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Using GPU: NVIDIA H100 80GB HBM3\n",
            "GPU Memory: 85.0 GB\n"
          ]
        }
      ],
      "source": [
        "# Set device - automatically use GPU if available, otherwise CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"🚀 Using GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"🐌 Using CPU (this will be slower)\")\n",
        "    print(\"💡 Tip: Consider using a machine with GPU for faster inference\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run EUV Spectra Prediction (Easy Method)\n",
        "\n",
        "This is the simplest way to run inference. The `run_inference` function handles everything for you and will show spectral predictions vs ground truth!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔬 Starting EUV spectra prediction inference...\n",
            "Loading model from ./assets/euv_spectra_weights.pth\n",
            "Initializing HelioSpectformer1D.\n",
            "GPU is available\n",
            "Loading pretrained model from ../../data/Surya-1.0/surya.366m.v1.pt.\n",
            "5 parameters require gradients: cls_token, linear.weight, linear.bias, unembed.weight, unembed.bias.\n",
            "Failed to load with strict=True: Error(s) in loading state_dict for HelioSpectformer1D:\n",
            "\tMissing key(s) in state_dict: \"cls_token\", \"embedding.pos_embed\", \"embedding.patch_embed.proj.weight\", \"embedding.patch_embed.proj.bias\", \"backbone.blocks_spectral_gating.0.norm1.weight\", \"backbone.blocks_spectral_gating.0.norm1.bias\", \"backbone.blocks_spectral_gating.0.filter.complex_weight\", \"backbone.blocks_spectral_gating.0.norm2.weight\", \"backbone.blocks_spectral_gating.0.norm2.bias\", \"backbone.blocks_spectral_gating.0.mlp.fc1.weight\", \"backbone.blocks_spectral_gating.0.mlp.fc1.bias\", \"backbone.blocks_spectral_gating.0.mlp.fc2.weight\", \"backbone.blocks_spectral_gating.0.mlp.fc2.bias\", \"backbone.blocks_spectral_gating.1.norm1.weight\", \"backbone.blocks_spectral_gating.1.norm1.bias\", \"backbone.blocks_spectral_gating.1.filter.complex_weight\", \"backbone.blocks_spectral_gating.1.norm2.weight\", \"backbone.blocks_spectral_gating.1.norm2.bias\", \"backbone.blocks_spectral_gating.1.mlp.fc1.weight\", \"backbone.blocks_spectral_gating.1.mlp.fc1.bias\", \"backbone.blocks_spectral_gating.1.mlp.fc2.weight\", \"backbone.blocks_spectral_gating.1.mlp.fc2.bias\", \"backbone.blocks_attention.0.norm1.weight\", \"backbone.blocks_attention.0.norm1.bias\", \"backbone.blocks_attention.0.norm2.weight\", \"backbone.blocks_attention.0.norm2.bias\", \"backbone.blocks_attention.0.mlp.fc1.weight\", \"backbone.blocks_attention.0.mlp.fc1.bias\", \"backbone.blocks_attention.0.mlp.fc2.weight\", \"backbone.blocks_attention.0.mlp.fc2.bias\", \"backbone.blocks_attention.0.attn.qkv.weight\", \"backbone.blocks_attention.0.attn.proj.weight\", \"backbone.blocks_attention.0.attn.proj.bias\", \"backbone.blocks_attention.0.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.0.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.0.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.0.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.0.attn.dual_ln_full.weight\", \"backbone.blocks_attention.0.attn.dual_ln_full.bias\", \"backbone.blocks_attention.1.norm1.weight\", \"backbone.blocks_attention.1.norm1.bias\", \"backbone.blocks_attention.1.norm2.weight\", \"backbone.blocks_attention.1.norm2.bias\", \"backbone.blocks_attention.1.mlp.fc1.weight\", \"backbone.blocks_attention.1.mlp.fc1.bias\", \"backbone.blocks_attention.1.mlp.fc2.weight\", \"backbone.blocks_attention.1.mlp.fc2.bias\", \"backbone.blocks_attention.1.attn.qkv.weight\", \"backbone.blocks_attention.1.attn.proj.weight\", \"backbone.blocks_attention.1.attn.proj.bias\", \"backbone.blocks_attention.1.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.1.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.1.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.1.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.1.attn.dual_ln_full.weight\", \"backbone.blocks_attention.1.attn.dual_ln_full.bias\", \"backbone.blocks_attention.2.norm1.weight\", \"backbone.blocks_attention.2.norm1.bias\", \"backbone.blocks_attention.2.norm2.weight\", \"backbone.blocks_attention.2.norm2.bias\", \"backbone.blocks_attention.2.mlp.fc1.weight\", \"backbone.blocks_attention.2.mlp.fc1.bias\", \"backbone.blocks_attention.2.mlp.fc2.weight\", \"backbone.blocks_attention.2.mlp.fc2.bias\", \"backbone.blocks_attention.2.attn.qkv.weight\", \"backbone.blocks_attention.2.attn.proj.weight\", \"backbone.blocks_attention.2.attn.proj.bias\", \"backbone.blocks_attention.2.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.2.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.2.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.2.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.2.attn.dual_ln_full.weight\", \"backbone.blocks_attention.2.attn.dual_ln_full.bias\", \"backbone.blocks_attention.3.norm1.weight\", \"backbone.blocks_attention.3.norm1.bias\", \"backbone.blocks_attention.3.norm2.weight\", \"backbone.blocks_attention.3.norm2.bias\", \"backbone.blocks_attention.3.mlp.fc1.weight\", \"backbone.blocks_attention.3.mlp.fc1.bias\", \"backbone.blocks_attention.3.mlp.fc2.weight\", \"backbone.blocks_attention.3.mlp.fc2.bias\", \"backbone.blocks_attention.3.attn.qkv.weight\", \"backbone.blocks_attention.3.attn.proj.weight\", \"backbone.blocks_attention.3.attn.proj.bias\", \"backbone.blocks_attention.3.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.3.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.3.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.3.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.3.attn.dual_ln_full.weight\", \"backbone.blocks_attention.3.attn.dual_ln_full.bias\", \"backbone.blocks_attention.4.norm1.weight\", \"backbone.blocks_attention.4.norm1.bias\", \"backbone.blocks_attention.4.norm2.weight\", \"backbone.blocks_attention.4.norm2.bias\", \"backbone.blocks_attention.4.mlp.fc1.weight\", \"backbone.blocks_attention.4.mlp.fc1.bias\", \"backbone.blocks_attention.4.mlp.fc2.weight\", \"backbone.blocks_attention.4.mlp.fc2.bias\", \"backbone.blocks_attention.4.attn.qkv.weight\", \"backbone.blocks_attention.4.attn.proj.weight\", \"backbone.blocks_attention.4.attn.proj.bias\", \"backbone.blocks_attention.4.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.4.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.4.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.4.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.4.attn.dual_ln_full.weight\", \"backbone.blocks_attention.4.attn.dual_ln_full.bias\", \"backbone.blocks_attention.5.norm1.weight\", \"backbone.blocks_attention.5.norm1.bias\", \"backbone.blocks_attention.5.norm2.weight\", \"backbone.blocks_attention.5.norm2.bias\", \"backbone.blocks_attention.5.mlp.fc1.weight\", \"backbone.blocks_attention.5.mlp.fc1.bias\", \"backbone.blocks_attention.5.mlp.fc2.weight\", \"backbone.blocks_attention.5.mlp.fc2.bias\", \"backbone.blocks_attention.5.attn.qkv.weight\", \"backbone.blocks_attention.5.attn.proj.weight\", \"backbone.blocks_attention.5.attn.proj.bias\", \"backbone.blocks_attention.5.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.5.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.5.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.5.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.5.attn.dual_ln_full.weight\", \"backbone.blocks_attention.5.attn.dual_ln_full.bias\", \"backbone.blocks_attention.6.norm1.weight\", \"backbone.blocks_attention.6.norm1.bias\", \"backbone.blocks_attention.6.norm2.weight\", \"backbone.blocks_attention.6.norm2.bias\", \"backbone.blocks_attention.6.mlp.fc1.weight\", \"backbone.blocks_attention.6.mlp.fc1.bias\", \"backbone.blocks_attention.6.mlp.fc2.weight\", \"backbone.blocks_attention.6.mlp.fc2.bias\", \"backbone.blocks_attention.6.attn.qkv.weight\", \"backbone.blocks_attention.6.attn.proj.weight\", \"backbone.blocks_attention.6.attn.proj.bias\", \"backbone.blocks_attention.6.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.6.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.6.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.6.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.6.attn.dual_ln_full.weight\", \"backbone.blocks_attention.6.attn.dual_ln_full.bias\", \"backbone.blocks_attention.7.norm1.weight\", \"backbone.blocks_attention.7.norm1.bias\", \"backbone.blocks_attention.7.norm2.weight\", \"backbone.blocks_attention.7.norm2.bias\", \"backbone.blocks_attention.7.mlp.fc1.weight\", \"backbone.blocks_attention.7.mlp.fc1.bias\", \"backbone.blocks_attention.7.mlp.fc2.weight\", \"backbone.blocks_attention.7.mlp.fc2.bias\", \"backbone.blocks_attention.7.attn.qkv.weight\", \"backbone.blocks_attention.7.attn.proj.weight\", \"backbone.blocks_attention.7.attn.proj.bias\", \"backbone.blocks_attention.7.attn.to_dynamic_projection.weight\", \"backbone.blocks_attention.7.attn.to_dynamic_projection.bias\", \"backbone.blocks_attention.7.attn.dual_ln_dp.weight\", \"backbone.blocks_attention.7.attn.dual_ln_dp.bias\", \"backbone.blocks_attention.7.attn.dual_ln_full.weight\", \"backbone.blocks_attention.7.attn.dual_ln_full.bias\", \"linear.weight\", \"linear.bias\", \"unembed.weight\", \"unembed.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"base_model.model.cls_token\", \"base_model.model.embedding.pos_embed\", \"base_model.model.embedding.patch_embed.proj.weight\", \"base_model.model.embedding.patch_embed.proj.bias\", \"base_model.model.backbone.blocks_spectral_gating.0.norm1.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.norm1.bias\", \"base_model.model.backbone.blocks_spectral_gating.0.filter.complex_weight\", \"base_model.model.backbone.blocks_spectral_gating.0.norm2.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.norm2.bias\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.norm1.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.norm1.bias\", \"base_model.model.backbone.blocks_spectral_gating.1.filter.complex_weight\", \"base_model.model.backbone.blocks_spectral_gating.1.norm2.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.norm2.bias\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.0.norm1.weight\", \"base_model.model.backbone.blocks_attention.0.norm1.bias\", \"base_model.model.backbone.blocks_attention.0.norm2.weight\", \"base_model.model.backbone.blocks_attention.0.norm2.bias\", \"base_model.model.backbone.blocks_attention.0.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.0.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.0.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.0.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.0.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.0.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.0.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.0.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.0.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.0.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.0.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.0.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.0.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.0.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.0.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.0.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.0.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.1.norm1.weight\", \"base_model.model.backbone.blocks_attention.1.norm1.bias\", \"base_model.model.backbone.blocks_attention.1.norm2.weight\", \"base_model.model.backbone.blocks_attention.1.norm2.bias\", \"base_model.model.backbone.blocks_attention.1.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.1.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.1.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.1.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.1.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.1.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.1.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.1.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.1.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.1.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.1.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.1.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.1.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.1.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.1.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.1.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.1.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.2.norm1.weight\", \"base_model.model.backbone.blocks_attention.2.norm1.bias\", \"base_model.model.backbone.blocks_attention.2.norm2.weight\", \"base_model.model.backbone.blocks_attention.2.norm2.bias\", \"base_model.model.backbone.blocks_attention.2.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.2.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.2.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.2.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.2.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.2.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.2.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.2.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.2.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.2.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.2.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.2.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.2.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.2.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.2.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.2.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.2.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.3.norm1.weight\", \"base_model.model.backbone.blocks_attention.3.norm1.bias\", \"base_model.model.backbone.blocks_attention.3.norm2.weight\", \"base_model.model.backbone.blocks_attention.3.norm2.bias\", \"base_model.model.backbone.blocks_attention.3.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.3.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.3.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.3.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.3.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.3.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.3.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.3.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.3.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.3.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.3.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.3.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.3.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.3.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.3.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.3.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.3.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.4.norm1.weight\", \"base_model.model.backbone.blocks_attention.4.norm1.bias\", \"base_model.model.backbone.blocks_attention.4.norm2.weight\", \"base_model.model.backbone.blocks_attention.4.norm2.bias\", \"base_model.model.backbone.blocks_attention.4.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.4.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.4.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.4.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.4.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.4.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.4.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.4.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.4.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.4.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.4.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.4.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.4.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.4.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.4.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.4.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.4.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.5.norm1.weight\", \"base_model.model.backbone.blocks_attention.5.norm1.bias\", \"base_model.model.backbone.blocks_attention.5.norm2.weight\", \"base_model.model.backbone.blocks_attention.5.norm2.bias\", \"base_model.model.backbone.blocks_attention.5.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.5.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.5.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.5.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.5.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.5.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.5.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.5.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.5.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.5.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.5.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.5.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.5.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.5.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.5.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.5.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.5.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.6.norm1.weight\", \"base_model.model.backbone.blocks_attention.6.norm1.bias\", \"base_model.model.backbone.blocks_attention.6.norm2.weight\", \"base_model.model.backbone.blocks_attention.6.norm2.bias\", \"base_model.model.backbone.blocks_attention.6.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.6.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.6.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.6.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.6.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.6.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.6.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.6.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.6.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.6.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.6.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.6.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.6.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.6.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.6.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.6.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.6.attn.dual_ln_full.bias\", \"base_model.model.backbone.blocks_attention.7.norm1.weight\", \"base_model.model.backbone.blocks_attention.7.norm1.bias\", \"base_model.model.backbone.blocks_attention.7.norm2.weight\", \"base_model.model.backbone.blocks_attention.7.norm2.bias\", \"base_model.model.backbone.blocks_attention.7.mlp.fc1.base_layer.weight\", \"base_model.model.backbone.blocks_attention.7.mlp.fc1.base_layer.bias\", \"base_model.model.backbone.blocks_attention.7.mlp.fc1.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.7.mlp.fc1.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.7.mlp.fc2.base_layer.weight\", \"base_model.model.backbone.blocks_attention.7.mlp.fc2.base_layer.bias\", \"base_model.model.backbone.blocks_attention.7.mlp.fc2.lora_A.default.weight\", \"base_model.model.backbone.blocks_attention.7.mlp.fc2.lora_B.default.weight\", \"base_model.model.backbone.blocks_attention.7.attn.qkv.weight\", \"base_model.model.backbone.blocks_attention.7.attn.proj.weight\", \"base_model.model.backbone.blocks_attention.7.attn.proj.bias\", \"base_model.model.backbone.blocks_attention.7.attn.to_dynamic_projection.weight\", \"base_model.model.backbone.blocks_attention.7.attn.to_dynamic_projection.bias\", \"base_model.model.backbone.blocks_attention.7.attn.dual_ln_dp.weight\", \"base_model.model.backbone.blocks_attention.7.attn.dual_ln_dp.bias\", \"base_model.model.backbone.blocks_attention.7.attn.dual_ln_full.weight\", \"base_model.model.backbone.blocks_attention.7.attn.dual_ln_full.bias\", \"base_model.model.linear.weight\", \"base_model.model.linear.bias\", \"base_model.model.unembed.weight\", \"base_model.model.unembed.bias\". \n",
            "Missing keys: ['cls_token', 'embedding.pos_embed', 'embedding.patch_embed.proj.weight', 'embedding.patch_embed.proj.bias', 'backbone.blocks_spectral_gating.0.norm1.weight', 'backbone.blocks_spectral_gating.0.norm1.bias', 'backbone.blocks_spectral_gating.0.filter.complex_weight', 'backbone.blocks_spectral_gating.0.norm2.weight', 'backbone.blocks_spectral_gating.0.norm2.bias', 'backbone.blocks_spectral_gating.0.mlp.fc1.weight', 'backbone.blocks_spectral_gating.0.mlp.fc1.bias', 'backbone.blocks_spectral_gating.0.mlp.fc2.weight', 'backbone.blocks_spectral_gating.0.mlp.fc2.bias', 'backbone.blocks_spectral_gating.1.norm1.weight', 'backbone.blocks_spectral_gating.1.norm1.bias', 'backbone.blocks_spectral_gating.1.filter.complex_weight', 'backbone.blocks_spectral_gating.1.norm2.weight', 'backbone.blocks_spectral_gating.1.norm2.bias', 'backbone.blocks_spectral_gating.1.mlp.fc1.weight', 'backbone.blocks_spectral_gating.1.mlp.fc1.bias', 'backbone.blocks_spectral_gating.1.mlp.fc2.weight', 'backbone.blocks_spectral_gating.1.mlp.fc2.bias', 'backbone.blocks_attention.0.norm1.weight', 'backbone.blocks_attention.0.norm1.bias', 'backbone.blocks_attention.0.norm2.weight', 'backbone.blocks_attention.0.norm2.bias', 'backbone.blocks_attention.0.mlp.fc1.weight', 'backbone.blocks_attention.0.mlp.fc1.bias', 'backbone.blocks_attention.0.mlp.fc2.weight', 'backbone.blocks_attention.0.mlp.fc2.bias', 'backbone.blocks_attention.0.attn.qkv.weight', 'backbone.blocks_attention.0.attn.proj.weight', 'backbone.blocks_attention.0.attn.proj.bias', 'backbone.blocks_attention.0.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.0.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.0.attn.dual_ln_dp.weight', 'backbone.blocks_attention.0.attn.dual_ln_dp.bias', 'backbone.blocks_attention.0.attn.dual_ln_full.weight', 'backbone.blocks_attention.0.attn.dual_ln_full.bias', 'backbone.blocks_attention.1.norm1.weight', 'backbone.blocks_attention.1.norm1.bias', 'backbone.blocks_attention.1.norm2.weight', 'backbone.blocks_attention.1.norm2.bias', 'backbone.blocks_attention.1.mlp.fc1.weight', 'backbone.blocks_attention.1.mlp.fc1.bias', 'backbone.blocks_attention.1.mlp.fc2.weight', 'backbone.blocks_attention.1.mlp.fc2.bias', 'backbone.blocks_attention.1.attn.qkv.weight', 'backbone.blocks_attention.1.attn.proj.weight', 'backbone.blocks_attention.1.attn.proj.bias', 'backbone.blocks_attention.1.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.1.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.1.attn.dual_ln_dp.weight', 'backbone.blocks_attention.1.attn.dual_ln_dp.bias', 'backbone.blocks_attention.1.attn.dual_ln_full.weight', 'backbone.blocks_attention.1.attn.dual_ln_full.bias', 'backbone.blocks_attention.2.norm1.weight', 'backbone.blocks_attention.2.norm1.bias', 'backbone.blocks_attention.2.norm2.weight', 'backbone.blocks_attention.2.norm2.bias', 'backbone.blocks_attention.2.mlp.fc1.weight', 'backbone.blocks_attention.2.mlp.fc1.bias', 'backbone.blocks_attention.2.mlp.fc2.weight', 'backbone.blocks_attention.2.mlp.fc2.bias', 'backbone.blocks_attention.2.attn.qkv.weight', 'backbone.blocks_attention.2.attn.proj.weight', 'backbone.blocks_attention.2.attn.proj.bias', 'backbone.blocks_attention.2.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.2.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.2.attn.dual_ln_dp.weight', 'backbone.blocks_attention.2.attn.dual_ln_dp.bias', 'backbone.blocks_attention.2.attn.dual_ln_full.weight', 'backbone.blocks_attention.2.attn.dual_ln_full.bias', 'backbone.blocks_attention.3.norm1.weight', 'backbone.blocks_attention.3.norm1.bias', 'backbone.blocks_attention.3.norm2.weight', 'backbone.blocks_attention.3.norm2.bias', 'backbone.blocks_attention.3.mlp.fc1.weight', 'backbone.blocks_attention.3.mlp.fc1.bias', 'backbone.blocks_attention.3.mlp.fc2.weight', 'backbone.blocks_attention.3.mlp.fc2.bias', 'backbone.blocks_attention.3.attn.qkv.weight', 'backbone.blocks_attention.3.attn.proj.weight', 'backbone.blocks_attention.3.attn.proj.bias', 'backbone.blocks_attention.3.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.3.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.3.attn.dual_ln_dp.weight', 'backbone.blocks_attention.3.attn.dual_ln_dp.bias', 'backbone.blocks_attention.3.attn.dual_ln_full.weight', 'backbone.blocks_attention.3.attn.dual_ln_full.bias', 'backbone.blocks_attention.4.norm1.weight', 'backbone.blocks_attention.4.norm1.bias', 'backbone.blocks_attention.4.norm2.weight', 'backbone.blocks_attention.4.norm2.bias', 'backbone.blocks_attention.4.mlp.fc1.weight', 'backbone.blocks_attention.4.mlp.fc1.bias', 'backbone.blocks_attention.4.mlp.fc2.weight', 'backbone.blocks_attention.4.mlp.fc2.bias', 'backbone.blocks_attention.4.attn.qkv.weight', 'backbone.blocks_attention.4.attn.proj.weight', 'backbone.blocks_attention.4.attn.proj.bias', 'backbone.blocks_attention.4.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.4.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.4.attn.dual_ln_dp.weight', 'backbone.blocks_attention.4.attn.dual_ln_dp.bias', 'backbone.blocks_attention.4.attn.dual_ln_full.weight', 'backbone.blocks_attention.4.attn.dual_ln_full.bias', 'backbone.blocks_attention.5.norm1.weight', 'backbone.blocks_attention.5.norm1.bias', 'backbone.blocks_attention.5.norm2.weight', 'backbone.blocks_attention.5.norm2.bias', 'backbone.blocks_attention.5.mlp.fc1.weight', 'backbone.blocks_attention.5.mlp.fc1.bias', 'backbone.blocks_attention.5.mlp.fc2.weight', 'backbone.blocks_attention.5.mlp.fc2.bias', 'backbone.blocks_attention.5.attn.qkv.weight', 'backbone.blocks_attention.5.attn.proj.weight', 'backbone.blocks_attention.5.attn.proj.bias', 'backbone.blocks_attention.5.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.5.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.5.attn.dual_ln_dp.weight', 'backbone.blocks_attention.5.attn.dual_ln_dp.bias', 'backbone.blocks_attention.5.attn.dual_ln_full.weight', 'backbone.blocks_attention.5.attn.dual_ln_full.bias', 'backbone.blocks_attention.6.norm1.weight', 'backbone.blocks_attention.6.norm1.bias', 'backbone.blocks_attention.6.norm2.weight', 'backbone.blocks_attention.6.norm2.bias', 'backbone.blocks_attention.6.mlp.fc1.weight', 'backbone.blocks_attention.6.mlp.fc1.bias', 'backbone.blocks_attention.6.mlp.fc2.weight', 'backbone.blocks_attention.6.mlp.fc2.bias', 'backbone.blocks_attention.6.attn.qkv.weight', 'backbone.blocks_attention.6.attn.proj.weight', 'backbone.blocks_attention.6.attn.proj.bias', 'backbone.blocks_attention.6.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.6.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.6.attn.dual_ln_dp.weight', 'backbone.blocks_attention.6.attn.dual_ln_dp.bias', 'backbone.blocks_attention.6.attn.dual_ln_full.weight', 'backbone.blocks_attention.6.attn.dual_ln_full.bias', 'backbone.blocks_attention.7.norm1.weight', 'backbone.blocks_attention.7.norm1.bias', 'backbone.blocks_attention.7.norm2.weight', 'backbone.blocks_attention.7.norm2.bias', 'backbone.blocks_attention.7.mlp.fc1.weight', 'backbone.blocks_attention.7.mlp.fc1.bias', 'backbone.blocks_attention.7.mlp.fc2.weight', 'backbone.blocks_attention.7.mlp.fc2.bias', 'backbone.blocks_attention.7.attn.qkv.weight', 'backbone.blocks_attention.7.attn.proj.weight', 'backbone.blocks_attention.7.attn.proj.bias', 'backbone.blocks_attention.7.attn.to_dynamic_projection.weight', 'backbone.blocks_attention.7.attn.to_dynamic_projection.bias', 'backbone.blocks_attention.7.attn.dual_ln_dp.weight', 'backbone.blocks_attention.7.attn.dual_ln_dp.bias', 'backbone.blocks_attention.7.attn.dual_ln_full.weight', 'backbone.blocks_attention.7.attn.dual_ln_full.bias', 'linear.weight', 'linear.bias', 'unembed.weight', 'unembed.bias']\n",
            "Unexpected keys: ['base_model.model.cls_token', 'base_model.model.embedding.pos_embed', 'base_model.model.embedding.patch_embed.proj.weight', 'base_model.model.embedding.patch_embed.proj.bias', 'base_model.model.backbone.blocks_spectral_gating.0.norm1.weight', 'base_model.model.backbone.blocks_spectral_gating.0.norm1.bias', 'base_model.model.backbone.blocks_spectral_gating.0.filter.complex_weight', 'base_model.model.backbone.blocks_spectral_gating.0.norm2.weight', 'base_model.model.backbone.blocks_spectral_gating.0.norm2.bias', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_spectral_gating.0.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_spectral_gating.1.norm1.weight', 'base_model.model.backbone.blocks_spectral_gating.1.norm1.bias', 'base_model.model.backbone.blocks_spectral_gating.1.filter.complex_weight', 'base_model.model.backbone.blocks_spectral_gating.1.norm2.weight', 'base_model.model.backbone.blocks_spectral_gating.1.norm2.bias', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_spectral_gating.1.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.0.norm1.weight', 'base_model.model.backbone.blocks_attention.0.norm1.bias', 'base_model.model.backbone.blocks_attention.0.norm2.weight', 'base_model.model.backbone.blocks_attention.0.norm2.bias', 'base_model.model.backbone.blocks_attention.0.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.0.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.0.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.0.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.0.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.0.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.0.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.0.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.0.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.0.attn.proj.weight', 'base_model.model.backbone.blocks_attention.0.attn.proj.bias', 'base_model.model.backbone.blocks_attention.0.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.0.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.0.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.0.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.0.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.0.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.1.norm1.weight', 'base_model.model.backbone.blocks_attention.1.norm1.bias', 'base_model.model.backbone.blocks_attention.1.norm2.weight', 'base_model.model.backbone.blocks_attention.1.norm2.bias', 'base_model.model.backbone.blocks_attention.1.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.1.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.1.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.1.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.1.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.1.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.1.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.1.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.1.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.1.attn.proj.weight', 'base_model.model.backbone.blocks_attention.1.attn.proj.bias', 'base_model.model.backbone.blocks_attention.1.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.1.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.1.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.1.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.1.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.1.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.2.norm1.weight', 'base_model.model.backbone.blocks_attention.2.norm1.bias', 'base_model.model.backbone.blocks_attention.2.norm2.weight', 'base_model.model.backbone.blocks_attention.2.norm2.bias', 'base_model.model.backbone.blocks_attention.2.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.2.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.2.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.2.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.2.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.2.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.2.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.2.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.2.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.2.attn.proj.weight', 'base_model.model.backbone.blocks_attention.2.attn.proj.bias', 'base_model.model.backbone.blocks_attention.2.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.2.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.2.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.2.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.2.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.2.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.3.norm1.weight', 'base_model.model.backbone.blocks_attention.3.norm1.bias', 'base_model.model.backbone.blocks_attention.3.norm2.weight', 'base_model.model.backbone.blocks_attention.3.norm2.bias', 'base_model.model.backbone.blocks_attention.3.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.3.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.3.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.3.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.3.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.3.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.3.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.3.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.3.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.3.attn.proj.weight', 'base_model.model.backbone.blocks_attention.3.attn.proj.bias', 'base_model.model.backbone.blocks_attention.3.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.3.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.3.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.3.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.3.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.3.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.4.norm1.weight', 'base_model.model.backbone.blocks_attention.4.norm1.bias', 'base_model.model.backbone.blocks_attention.4.norm2.weight', 'base_model.model.backbone.blocks_attention.4.norm2.bias', 'base_model.model.backbone.blocks_attention.4.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.4.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.4.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.4.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.4.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.4.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.4.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.4.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.4.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.4.attn.proj.weight', 'base_model.model.backbone.blocks_attention.4.attn.proj.bias', 'base_model.model.backbone.blocks_attention.4.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.4.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.4.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.4.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.4.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.4.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.5.norm1.weight', 'base_model.model.backbone.blocks_attention.5.norm1.bias', 'base_model.model.backbone.blocks_attention.5.norm2.weight', 'base_model.model.backbone.blocks_attention.5.norm2.bias', 'base_model.model.backbone.blocks_attention.5.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.5.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.5.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.5.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.5.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.5.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.5.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.5.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.5.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.5.attn.proj.weight', 'base_model.model.backbone.blocks_attention.5.attn.proj.bias', 'base_model.model.backbone.blocks_attention.5.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.5.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.5.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.5.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.5.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.5.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.6.norm1.weight', 'base_model.model.backbone.blocks_attention.6.norm1.bias', 'base_model.model.backbone.blocks_attention.6.norm2.weight', 'base_model.model.backbone.blocks_attention.6.norm2.bias', 'base_model.model.backbone.blocks_attention.6.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.6.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.6.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.6.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.6.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.6.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.6.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.6.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.6.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.6.attn.proj.weight', 'base_model.model.backbone.blocks_attention.6.attn.proj.bias', 'base_model.model.backbone.blocks_attention.6.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.6.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.6.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.6.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.6.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.6.attn.dual_ln_full.bias', 'base_model.model.backbone.blocks_attention.7.norm1.weight', 'base_model.model.backbone.blocks_attention.7.norm1.bias', 'base_model.model.backbone.blocks_attention.7.norm2.weight', 'base_model.model.backbone.blocks_attention.7.norm2.bias', 'base_model.model.backbone.blocks_attention.7.mlp.fc1.base_layer.weight', 'base_model.model.backbone.blocks_attention.7.mlp.fc1.base_layer.bias', 'base_model.model.backbone.blocks_attention.7.mlp.fc1.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.7.mlp.fc1.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.7.mlp.fc2.base_layer.weight', 'base_model.model.backbone.blocks_attention.7.mlp.fc2.base_layer.bias', 'base_model.model.backbone.blocks_attention.7.mlp.fc2.lora_A.default.weight', 'base_model.model.backbone.blocks_attention.7.mlp.fc2.lora_B.default.weight', 'base_model.model.backbone.blocks_attention.7.attn.qkv.weight', 'base_model.model.backbone.blocks_attention.7.attn.proj.weight', 'base_model.model.backbone.blocks_attention.7.attn.proj.bias', 'base_model.model.backbone.blocks_attention.7.attn.to_dynamic_projection.weight', 'base_model.model.backbone.blocks_attention.7.attn.to_dynamic_projection.bias', 'base_model.model.backbone.blocks_attention.7.attn.dual_ln_dp.weight', 'base_model.model.backbone.blocks_attention.7.attn.dual_ln_dp.bias', 'base_model.model.backbone.blocks_attention.7.attn.dual_ln_full.weight', 'base_model.model.backbone.blocks_attention.7.attn.dual_ln_full.bias', 'base_model.model.linear.weight', 'base_model.model.linear.bias', 'base_model.model.unembed.weight', 'base_model.model.unembed.bias']\n",
            "Dataset size: 3\n",
            "\n",
            "====================================================================================================\n",
            "Sample 1\n",
            "Time Input           | Time Target          | Spectrum Shape       | Pred Range                | GT Range                 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "2011-01-07T07:12     | 2011-01-07T08:12     | 1343                 | [0.1595, 0.8113]:<25 | [0.2851, 0.9661]:<25\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Sample 2\n",
            "Time Input           | Time Target          | Spectrum Shape       | Pred Range                | GT Range                 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "2011-01-07T07:00     | 2011-01-07T08:00     | 1343                 | [0.1699, 0.8427]:<25 | [0.3725, 0.9662]:<25\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Sample 3\n",
            "Time Input           | Time Target          | Spectrum Shape       | Pred Range                | GT Range                 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "2011-01-07T06:48     | 2011-01-07T07:48     | 1343                 | [0.1607, 0.8120]:<25 | [0.3986, 0.9658]:<25\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "SUMMARY STATISTICS\n",
            "====================================================================================================\n",
            "Metric                              | Value               \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mean Absolute Error                 | 0.151952            \n",
            "Root Mean Square Error              | 0.187314            \n",
            "R² Score                            | -6.409858           \n",
            "Average Spectral Correlation        | -0.033348           \n",
            "Number of Samples                   | 3                   \n",
            "Spectrum Dimension                  | 1343                \n",
            "====================================================================================================\n",
            "\n",
            "📊 Visualization saved to: ./inference_results/euv_spectra_predictions.png\n",
            "🎉 EUV spectra prediction completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Parameters for inference\n",
        "data_type = \"test\"  # or \"valid\" or \"train\" - which dataset to use\n",
        "num_viz_samples = 3  # Number of samples to process and analyze\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"🔬 Starting EUV spectra prediction inference...\")\n",
        "# Run the complete inference pipeline\n",
        "try:\n",
        "    run_inference(\n",
        "        config=config,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        output_dir=output_dir,\n",
        "        device=device,\n",
        "        data_type=data_type,\n",
        "        num_viz_samples=num_viz_samples,\n",
        "        device_type=device_type\n",
        "    )\n",
        "    print(\"🎉 EUV spectra prediction completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during inference: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding the Results 📊\n",
        "\n",
        "### What you're seeing:\n",
        "- **Time Input**: The timestamp of the input solar observations\n",
        "- **Time Target**: The timestamp for which we're making the spectral prediction\n",
        "- **Spectrum Shape**: Always 1343 (number of wavelength bins)\n",
        "- **Pred Range**: Min/max values in the predicted spectrum\n",
        "- **GT Range**: Min/max values in the ground truth spectrum\n",
        "\n",
        "### Key Metrics Explained:\n",
        "1. **Mean Absolute Error (MAE)**: Average absolute difference between predicted and actual spectral values (lower is better)\n",
        "2. **Root Mean Square Error (RMSE)**: Square root of average squared differences (penalizes larger errors more)\n",
        "3. **R² Score**: Coefficient of determination (closer to 1.0 means better predictions)\n",
        "4. **Average Spectral Correlation**: How well the predicted spectrum shape matches ground truth (closer to 1.0 is better)\n",
        "\n",
        "### Tips for interpretation:\n",
        "1. **Good predictions**: High correlation (>0.9) and low MAE (<0.05)\n",
        "2. **Spectral shape**: More important than absolute values - EUV lines should be in the right places\n",
        "3. **Physical meaning**: Each wavelength corresponds to different coronal temperatures and plasma conditions\n",
        "4. **Applications**: Space weather forecasting, solar physics research, understanding coronal heating\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary 🎯\n",
        "\n",
        "Congratulations! You've successfully run EUV spectra prediction using the Surya model. \n",
        "\n",
        "### What you accomplished:\n",
        "✅ Downloaded pre-trained model weights  \n",
        "✅ Loaded and configured the spectral regression model  \n",
        "✅ Ran inference on solar data  \n",
        "✅ Generated 1343-dimensional spectral predictions with timestamps  \n",
        "✅ Compared predictions with ground truth measurements  \n",
        "✅ Calculated regression metrics (MAE, RMSE, R², Correlation)  \n",
        "✅ Created spectral visualization plots  \n",
        "\n",
        "### Understanding EUV Spectra Prediction:\n",
        "- **Vector Regression Task**: Predicts 1343 continuous spectral values simultaneously\n",
        "- **Wavelength Coverage**: 6.5 to 33.3 nanometers (extreme ultraviolet range)\n",
        "- **Physical Significance**: Each wavelength reveals different aspects of solar coronal conditions\n",
        "- **Practical Applications**: Space weather monitoring, solar physics research, coronal diagnostics\n",
        "- **Challenges**: High dimensionality, maintaining spectral coherence, handling dynamic range\n",
        "\n",
        "### Performance Expectations:\n",
        "- **Excellent Model**: MAE < 0.03, RMSE < 0.05, R² > 0.9, Correlation > 0.95\n",
        "- **Good Model**: MAE < 0.05, RMSE < 0.08, R² > 0.8, Correlation > 0.9\n",
        "- **Acceptable Model**: MAE < 0.1, RMSE < 0.15, R² > 0.6, Correlation > 0.7\n",
        "- **Remember**: Spectral correlation is often more important than absolute error!\n",
        "\n",
        "### What makes this different from scalar prediction:\n",
        "- **High-dimensional output**: 1343 values instead of 1\n",
        "- **Spectral coherence**: Predictions must maintain realistic spectral line relationships\n",
        "- **Correlation analysis**: Shape matching is as important as absolute accuracy\n",
        "- **Physical constraints**: Each wavelength has specific physical meaning\n",
        "\n",
        "Happy EUV spectra forecasting! 🌞🔬📡✨\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
