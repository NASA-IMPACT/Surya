{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Flare Forecasting Tutorial for Beginners 🌞\n",
    "\n",
    "This notebook will guide you through running Solar Flare forecasting using the Surya model. \n",
    "\n",
    "## What you'll learn:\n",
    "- How to load a pre-trained model for solar flare prediction\n",
    "- How to run inference on solar data\n",
    "- How to interpret forecasting results\n",
    "\n",
    "## Prerequisites:\n",
    "- Make sure you're in the correct directory: `downstream_examples/solar_flare_forcasting/`\n",
    "- Ensure all required packages are installed (torch, yaml, matplotlib, numpy, etc.)\n",
    "\n",
    "Let's get started! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/rlal/Surya/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/rhome/rlal/Surya/.venv/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Import functions from our inference script\n",
    "from infer import (\n",
    "    load_model, \n",
    "    run_inference,\n",
    "    get_dataloader,\n",
    "    infer_single_sample\n",
    ")\n",
    "\n",
    "# Import from surya\n",
    "from surya.utils.data import build_scalers\n",
    "from surya.utils.distributed import set_global_seed\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Pre-trained Model Weights\n",
    "\n",
    "The model weights will be automatically downloaded from Hugging Face. This might take a few minutes on first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading model weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 398.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download model weights from Hugging Face\n",
    "print(\"📥 Downloading model weights...\")\n",
    "snapshot_download(\n",
    "    repo_id=\"nasa-ibm-ai4science/solar_flares_surya\",\n",
    "    local_dir=\"./assets\",\n",
    "    allow_patterns='*.pth',\n",
    "    token=None,\n",
    ")\n",
    "print(\"✅ Model weights downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Configuration\n",
    "\n",
    "We need to load the configuration file that contains all the model and data parameters. Make sure you have a `config.yaml` file in your current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Loading configuration...\n",
      "✅ Configuration loaded successfully!\n",
      "Model type: spectformer\n",
      "Data precision: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./config.yaml\"\n",
    "checkpoint_path = \"./assets/solar_flare_weights.pth\"\n",
    "output_dir = \"./inference_results\"\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "set_global_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "print(\"📋 Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"✅ Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "# Set data type (float precision)\n",
    "if config[\"dtype\"] == \"float16\":\n",
    "    config[\"dtype\"] = torch.float16\n",
    "elif config[\"dtype\"] == \"bfloat16\":\n",
    "    config[\"dtype\"] = torch.bfloat16\n",
    "elif config[\"dtype\"] == \"float32\":\n",
    "    config[\"dtype\"] = torch.float32\n",
    "else:\n",
    "    raise NotImplementedError(\"Please choose from [float16,bfloat16,float32]\")\n",
    "\n",
    "print(f\"Model type: {config['model']['model_type']}\")\n",
    "print(f\"Data precision: {config['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Device (GPU/CPU)\n",
    "\n",
    "Let's determine whether to use GPU or CPU for inference. GPU is much faster if available!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using GPU: NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Set device - automatically use GPU if available, otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"🚀 Using GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"🐌 Using CPU (this will be slower)\")\n",
    "    print(\"💡 Tip: Consider using a machine with GPU for faster inference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Run Solar Flare Forecasting (Easy Method)\n",
    "\n",
    "This is the simplest way to run inference. The `run_inference` function handles everything for you and will show predictions vs ground truth!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Starting solar flare forecasting inference...\n",
      "Loading model from ./assets/solar_flare_weights.pth\n",
      "GPU is available\n",
      "Loading pretrained model from ../../data/Surya-1.0/surya.366m.v1.pt.\n",
      "Applying PEFT LoRA with configuration: {'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2'], 'lora_dropout': 0.1, 'bias': 'none'}\n",
      "trainable params: 1,024,000 || all params: 364,593,153 || trainable%: 0.28%\n",
      "Dataset size: 3\n",
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2011-01-03T22:00     | 2011-01-04T00:00     | 0               | 0           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2011-01-04T16:00     | 2011-01-04T18:00     | 0               | 0           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2011-01-05T05:00     | 2011-01-05T07:00     | 0               | 0           \n",
      "================================================================================\n",
      "🎉 Solar flare forecasting completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Parameters for inference\n",
    "data_type = \"test\"  # or \"valid\" - which dataset to use\n",
    "num_samples = 3  # Number of samples to process and analyze\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"🔬 Starting solar flare forecasting inference...\")\n",
    "# Run the complete inference pipeline\n",
    "try:\n",
    "    run_inference(\n",
    "        config=config,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        output_dir=output_dir,\n",
    "        device=device,\n",
    "        data_type=data_type,\n",
    "        num_samples=num_samples,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    print(\"🎉 Solar flare forecasting completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during inference: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Understanding the Results 📊\n",
    "\n",
    "### What you're seeing:\n",
    "- **Time Input**: The timestamp of the input solar observations\n",
    "- **Time Target**: The timestamp for which we're making the flare prediction\n",
    "- **Prediction**: The model's binary prediction (0 = No Flare, 1 = Flare)\n",
    "- **Ground Truth**: The actual observed outcome (0 = No Flare, 1 = Flare)\n",
    "\n",
    "### Tips for interpretation:\n",
    "1. **Prediction = Ground Truth**: The model made a correct prediction\n",
    "2. **Prediction ≠ Ground Truth**: The model made an incorrect prediction\n",
    "3. **Time difference**: Shows how far ahead the model is forecasting\n",
    "4. **Multiple samples**: Each row shows a different solar observation and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Troubleshooting 🔧\n",
    "\n",
    "### Common issues:\n",
    "1. **\"No config.yaml found\"**: Make sure you have the configuration file in your directory\n",
    "2. **\"No data found\"**: Check that your data paths in config.yaml are correct\n",
    "3. **Import errors**: Ensure all required packages are installed\n",
    "4. **CUDA errors**: Make sure your GPU has enough memory or switch to CPU\n",
    "\n",
    "### Need help?\n",
    "- Check the original `infer.py` file for more details\n",
    "- Verify your data paths in the configuration\n",
    "- Make sure you're in the correct directory: `downstream_examples/solar_flare_forcasting/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary 🎯\n",
    "\n",
    "Congratulations! You've successfully run solar flare forecasting using the Surya model. \n",
    "\n",
    "### What you accomplished:\n",
    "✅ Downloaded pre-trained model weights  \n",
    "✅ Loaded and configured the model  \n",
    "✅ Ran inference on solar data  \n",
    "✅ Generated flare predictions with timestamps  \n",
    "✅ Compared predictions with ground truth  \n",
    "✅ Learned about manual inference options  \n",
    "\n",
    "### Next steps:\n",
    "- Try different numbers of samples\n",
    "- Experiment with different data types (test vs valid)\n",
    "- Analyze the prediction accuracy patterns\n",
    "- Check out the original `infer.py` for more advanced usage\n",
    "\n",
    "### Understanding Solar Flare Forecasting:\n",
    "- **Binary Classification**: The model predicts whether a solar flare will occur (1) or not (0)\n",
    "- **Time-based Prediction**: Uses current solar observations to predict future flare activity\n",
    "- **Practical Applications**: Helps protect satellites, power grids, and astronauts from space weather\n",
    "\n",
    "Happy solar flare forecasting! 🌞⚡✨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
